{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4341c594",
   "metadata": {},
   "source": [
    "Let's implement some basic classifiers\n",
    "- KNN\n",
    "- SVM\n",
    "- Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa3c5d9",
   "metadata": {},
   "source": [
    "# Loading test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd3d6ea",
   "metadata": {},
   "source": [
    "Let's use the Cifar10 dataset to test all these classifiers! Why? It's a classsic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a456254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Combine them for demo purposes\n",
    "X = np.concatenate((x_train, x_test))\n",
    "y = np.concatenate((y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b66df9a",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea87110",
   "metadata": {},
   "source": [
    "Let's first build a simple Nearest Neighbors classifier.\n",
    "\n",
    "A nearest neighbor classifier just computes the L1/L2 distance between an image and a set of images in its training class, assigning the output to the label of the image with the shortest distance. Let's implement it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1638c2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class NearestNeighbor():\n",
    "    def __init__(self,):\n",
    "        self.training_data = []\n",
    "        self.training_labels= []\n",
    "\n",
    "\n",
    "    # Yep, that's all it does during training\n",
    "    def train(self, X, y):\n",
    "        self.training_data = X\n",
    "        self.training_labels = y\n",
    "    \n",
    "    def compute_distance(self, X1, X2):\n",
    "        return np.sqrt(np.sum((X1-X2)**2))\n",
    "    \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        distances = np.array([[np.linalg.norm(x_test - x_train) for x_train in self.training_data] for x_test in X_test])\n",
    "        return np.array(self.training_labels)[distances.argmin(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06cbd4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 17.00%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[:2000], y[:2000], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train our Nearest Neighbor classifier\n",
    "nn = NearestNeighbor()\n",
    "nn.train(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = nn.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5416356f",
   "metadata": {},
   "source": [
    "Now let's do **K Nearest Neighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37ac0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class KNearestNeighbors():\n",
    "    def __init__(self,):\n",
    "        self.training_data = []\n",
    "        self.training_labels= []\n",
    "\n",
    "\n",
    "    # Yep, that's all it does during training\n",
    "    def train(self, X, y):\n",
    "        self.training_data = X\n",
    "        self.training_labels = y\n",
    "    \n",
    "    def compute_distance(self, X1, X2):\n",
    "        return np.sqrt(np.sum((X1-X2)**2))\n",
    "    \n",
    "    \n",
    "   \n",
    "    def predict(self, X_test, k=3):\n",
    "        predictions = []\n",
    "\n",
    "        for i in tqdm(range(len(X_test))):\n",
    "            dist_label_pairs = []\n",
    "\n",
    "            for j in range(len(self.training_data)):\n",
    "                dist = self.compute_distance(X_test[i], self.training_data[j])\n",
    "                dist_label_pairs.append((dist, self.training_labels[j]))\n",
    "\n",
    "            k_nearest = sorted(dist_label_pairs, key=lambda x: x[0])[:k]\n",
    "\n",
    "            label_count = {}\n",
    "            for _, label in k_nearest:\n",
    "                # Assuming label is a numpy.ndarray with a single value:\n",
    "                # Convert ndarray to a hashable type\n",
    "                if label.shape:  # Check if label is an array with elements\n",
    "                    label = label.item()  # Converts a one-element array to a scalar\n",
    "\n",
    "                if label in label_count:\n",
    "                    label_count[label] += 1\n",
    "                else:\n",
    "                    label_count[label] = 1\n",
    "\n",
    "            best_label = max(label_count, key=label_count.get)\n",
    "            predictions.append(best_label)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "555014df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 400/400 [00:06<00:00, 65.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 19.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[:2000], y[:2000], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train our Nearest Neighbor classifier\n",
    "knn = KNearestNeighbors()\n",
    "knn.train(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c811c2",
   "metadata": {},
   "source": [
    "That's slightly better! Let's move on to more advanced classifiers, specifically \n",
    "- Support Vector Machines\n",
    "- Softmax Classifiers\n",
    "\n",
    "They function in the same way (calculating wx + b and refining the layers), but with different loss values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea55da3b",
   "metadata": {},
   "source": [
    "# Generalized Structure for linear model classifiers\n",
    "\n",
    "We can define a class for both, which will share the general update structure, but utilize different loss + gradient calculations. The shared functionality will be in \n",
    "- Defining hyperparameters (learning rate, epochs)\n",
    "- Training\n",
    "- Predicting\n",
    "\n",
    "They will differ in their\n",
    "- Loss functions\n",
    "- Gradient functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c147d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class GeneralizedLinearModel:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, svm_loss_gradient=None):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.svm_loss_gradient = svm_loss_gradient\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        size, features = X.shape\n",
    "        X = np.hstack([X, np.ones((size, 1))])\n",
    "        \n",
    "        # Simple linear model (weights.shape = no. of features, bias.shape = 1)\n",
    "        self.weights = np.zeros(features + 1)\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            output = np.dot(X, self.weights) \n",
    "            \n",
    "            \n",
    "            loss, dw = svm_loss_gradient(self.weights, X, y, output, 0.01)\n",
    "            \n",
    "            self.weights -= dW * self.learning_rate   \n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "                \n",
    "    def predict(self, X):\n",
    "        X = np.hstack([X, np.ones((size, 1))])\n",
    "        return np.dot(X, self.weights) \n",
    "            \n",
    "\n",
    "def svm_loss_gradient(W, X, y, output, reg):\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # Get the scores of the correct labels\n",
    "    correct_scores = output[np.arange(N), y][:, np.newaxis]\n",
    "    \n",
    "    # Calculate margins\n",
    "    margins = np.maximum(0, output - correct_scores + delta)\n",
    "    margins[np.arange(N), y] = 0  # Do not consider correct class in margins\n",
    "\n",
    "    # Compute loss: data loss + regularization loss\n",
    "    loss = np.sum(margins) / N\n",
    "    loss += reg * np.sum(W * W)\n",
    "    \n",
    "    # Compute gradient\n",
    "    binary = margins > 0\n",
    "    binary[np.arange(N), y] = -np.sum(binary, axis=1)\n",
    "\n",
    "    dW = np.dot(X.T, binary)\n",
    "    dW /= N\n",
    "    dW += 2 * reg * W  # Gradient of regularization term\n",
    "\n",
    "    return loss, dW\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfbcb64",
   "metadata": {},
   "source": [
    "## SVM Loss\n",
    "Now we can define the SVM loss + gradient to use with the above structure and so on.\n",
    "\n",
    "Loss is $L_i = \\sum_{j \\neq y_i}\\text{max}(0,s_j-s_{y_j}+\\Delta)$\n",
    "- Get the classes\n",
    "- Calculate the hinge loss for each\n",
    "- Sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a0c2943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM uses hinge loss to define the loss\n",
    "\n",
    "# Predefine delta\n",
    "delta = 1\n",
    "\n",
    "def svm_loss_gradient(W, X, y, output, reg):\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # Get the scores of the correct labels\n",
    "    correct_scores = output[np.arange(N), y][:, np.newaxis]\n",
    "    \n",
    "    # Calculate margins\n",
    "    margins = np.maximum(0, output - correct_scores + delta)\n",
    "    margins[np.arange(N), y] = 0  # Do not consider correct class in margins\n",
    "\n",
    "    # Compute loss: data loss + regularization loss\n",
    "    loss = np.sum(margins) / N\n",
    "    loss += reg * np.sum(W * W)\n",
    "    \n",
    "    # Compute gradient\n",
    "    binary = margins > 0\n",
    "    binary[np.arange(N), y] = -np.sum(binary, axis=1)\n",
    "\n",
    "    dW = np.dot(X.T, binary)\n",
    "    dW /= N\n",
    "    dW += 2 * reg * W  # Gradient of regularization term\n",
    "\n",
    "    return loss, dW\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d03d165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[:2000], y[:2000], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "SVM = GeneralizedLinearModel(svm_loss_gradient=svm_loss_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad98009c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mSVM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 12\u001b[0m, in \u001b[0;36mGeneralizedLinearModel.train\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m---> 12\u001b[0m     size, features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     13\u001b[0m     X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([X, np\u001b[38;5;241m.\u001b[39mones((size, \u001b[38;5;241m1\u001b[39m))])\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Simple linear model (weights.shape = no. of features, bias.shape = 1)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "SVM.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "599cd7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 32, 32, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a1f1e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GeneralizedLinearModel:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, svm_loss_gradient=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.svm_loss_gradient = self.svm_loss\n",
    "        \n",
    "    def train(self, X, y):\n",
    "         \n",
    "        size = X.shape[0]\n",
    "        features = X.shape[1:]\n",
    "        \n",
    "        # Initialize weights (features + bias)\n",
    "        self.weights = np.zeros(features)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "#             output = np.dot(X, self.weights) \n",
    "            \n",
    "            loss, dW = self.svm_loss(self.weights, X, y, 0.01)\n",
    "            \n",
    "            self.weights -= dW * self.learning_rate\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "                \n",
    "    def predict(self, X):\n",
    "        size = X.shape[0]\n",
    "        X = np.hstack([X, np.ones((size, 1))])  # Add bias term\n",
    "        return np.dot(X, self.weights)\n",
    "\n",
    "    def svm_loss(self, W, X, y, reg):\n",
    "        loss = 0.0\n",
    "        dW = np.zeros(W.shape)  \n",
    "        print(W.shape, X.shape)\n",
    "\n",
    "        N = len(y)     # number of samples\n",
    "        Y_hat = X @ W  # raw scores matrix\n",
    "\n",
    "        y_hat_true = Y_hat[range(N), y][:, np.newaxis]    # scores for true labels\n",
    "        margins = np.maximum(0, Y_hat - y_hat_true + 1)   # margin for each score\n",
    "        loss = margins.sum() / N - 1 + reg * np.sum(W**2) # regularized loss\n",
    "\n",
    "        dW = (margins > 0).astype(int)    # initial gradient with respect to Y_hat\n",
    "        dW[range(N), y] -= dW.sum(axis=1) # update gradient to include correct labels\n",
    "        dW = X.T @ dW / N + 2 * reg * W   # gradient with respect to W\n",
    "\n",
    "        return loss, dW\n",
    "\n",
    "#     def default_svm_loss_gradient(self, W, X, y, output, reg):\n",
    "#         N = X.shape[0]\n",
    "#         delta = 1  # Delta value for hinge loss\n",
    "        \n",
    "#         correct_scores = output[np.arange(N), y][:, np.newaxis]\n",
    "#         margins = np.maximum(0, output - correct_scores + delta)\n",
    "#         margins[np.arange(N), y] = 0\n",
    "        \n",
    "#         loss = np.sum(margins) / N\n",
    "#         loss += reg * np.sum(W * W)\n",
    "        \n",
    "#         binary = margins > 0\n",
    "#         binary[np.arange(N), y] = -np.sum(binary, axis=1)\n",
    "\n",
    "#         dW = np.dot(X.T, binary)\n",
    "#         dW /= N\n",
    "#         dW += 2 * reg * W\n",
    "\n",
    "#         return loss, dW\n",
    "\n",
    "# Usage Example\n",
    "# model = GeneralizedLinearModel()\n",
    "# model.train(X_train, y_train)\n",
    "# predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ef56e828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072,) (60000, 3072)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Then use the flattened data for training\u001b[39;00m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m GeneralizedLinearModel()\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_flattened\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# predictions = model.predict(X_test)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[65], line 20\u001b[0m, in \u001b[0;36mGeneralizedLinearModel.train\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(features)\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#             output = np.dot(X, self.weights) \u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m             loss, dW \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvm_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m dW \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[65], line 40\u001b[0m, in \u001b[0;36mGeneralizedLinearModel.svm_loss\u001b[0;34m(self, W, X, y, reg)\u001b[0m\n\u001b[1;32m     37\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)     \u001b[38;5;66;03m# number of samples\u001b[39;00m\n\u001b[1;32m     38\u001b[0m Y_hat \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m@\u001b[39m W  \u001b[38;5;66;03m# raw scores matrix\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m y_hat_true \u001b[38;5;241m=\u001b[39m \u001b[43mY_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m[:, np\u001b[38;5;241m.\u001b[39mnewaxis]    \u001b[38;5;66;03m# scores for true labels\u001b[39;00m\n\u001b[1;32m     41\u001b[0m margins \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0\u001b[39m, Y_hat \u001b[38;5;241m-\u001b[39m y_hat_true \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)   \u001b[38;5;66;03m# margin for each score\u001b[39;00m\n\u001b[1;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m margins\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m N \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m reg \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(W\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# regularized loss\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "# X_train_flattened = X_train.reshape(X_train.shape[0], -1)  # Flatten the images\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the Iris dataset\n",
    "# iris = datasets.load_iris()\n",
    "# X_iris = iris.data  # Features\n",
    "# y_iris = iris.target  # Target labels\n",
    "\n",
    "# Now X_iris contains the features and y_iris contains the target labels\n",
    "X_flattened = X.reshape(X.shape[0], -1)  # This will change the shape to (60000, 3072)\n",
    "\n",
    "# Then use the flattened data for training\n",
    "model = GeneralizedLinearModel()\n",
    "model.train(X_flattened, y)\n",
    "# predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb354a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
