{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4341c594",
   "metadata": {},
   "source": [
    "Let's implement some basic classifiers\n",
    "- KNN\n",
    "- SVM\n",
    "- Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa3c5d9",
   "metadata": {},
   "source": [
    "# Loading test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd3d6ea",
   "metadata": {},
   "source": [
    "Let's use the Cifar10 dataset to test all these classifiers! Why? It's a classsic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a456254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Combine them for demo purposes\n",
    "X = np.concatenate((x_train, x_test))\n",
    "y = np.concatenate((y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b66df9a",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea87110",
   "metadata": {},
   "source": [
    "Let's first build a simple Nearest Neighbors classifier.\n",
    "\n",
    "A nearest neighbor classifier just computes the L1/L2 distance between an image and a set of images in its training class, assigning the output to the label of the image with the shortest distance. Let's implement it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1638c2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class NearestNeighbor():\n",
    "    def __init__(self,):\n",
    "        self.training_data = []\n",
    "        self.training_labels= []\n",
    "\n",
    "\n",
    "    # Yep, that's all it does during training\n",
    "    def train(self, X, y):\n",
    "        self.training_data = X\n",
    "        self.training_labels = y\n",
    "    \n",
    "    def compute_distance(self, X1, X2):\n",
    "        return np.sqrt(np.sum((X1-X2)**2))\n",
    "    \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        distances = np.array([[np.linalg.norm(x_test - x_train) for x_train in self.training_data] for x_test in X_test])\n",
    "        return np.array(self.training_labels)[distances.argmin(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cbd4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[:2000], y[:2000], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train our Nearest Neighbor classifier\n",
    "nn = NearestNeighbor()\n",
    "nn.train(X_train, y_train)\n",
    "\n",
    "# eval\n",
    "y_pred = nn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911db9f",
   "metadata": {},
   "source": [
    "Now let's do **K Nearest Neighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9563bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class KNearestNeighbors():\n",
    "    def __init__(self,):\n",
    "        self.training_data = []\n",
    "        self.training_labels= []\n",
    "\n",
    "\n",
    "    # Yep, that's all it does during training\n",
    "    def train(self, X, y):\n",
    "        self.training_data = X\n",
    "        self.training_labels = y\n",
    "    \n",
    "    def compute_distance(self, X1, X2):\n",
    "        return np.sqrt(np.sum((X1-X2)**2))\n",
    "    \n",
    "    \n",
    "   \n",
    "    def predict(self, X_test, k=3):\n",
    "        predictions = []\n",
    "\n",
    "\n",
    "        for i in tqdm(range(len(X_test))):\n",
    "            dist_label_pairs = []\n",
    "\n",
    "            for j in range(len(self.training_data)):\n",
    "                dist = self.compute_distance(X_test[i], self.training_data[j])\n",
    "                dist_label_pairs.append((dist, self.training_labels[j]))\n",
    "\n",
    "            k_nearest = sorted(dist_label_pairs, key=lambda x: x[0])[:k]\n",
    "\n",
    "            label_count = {}\n",
    "            for _, label in k_nearest:\n",
    "\n",
    "\n",
    "                if label.shape: \n",
    "                    label = label.item()  # Converts to scalar\n",
    "\n",
    "                if label in label_count:\n",
    "                    label_count[label] += 1\n",
    "                else:\n",
    "                    label_count[label] = 1\n",
    "\n",
    "            best_label = max(label_count, key=label_count.get)\n",
    "            predictions.append(best_label)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8338cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[:2000], y[:2000], test_size=0.2, random_state=42)\n",
    "\n",
    "# New KNN + train\n",
    "knn = KNearestNeighbors()\n",
    "knn.train(X_train, y_train)\n",
    "\n",
    "# Eval\n",
    "y_pred = knn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee3a12c",
   "metadata": {},
   "source": [
    "That's slightly better! Let's move on to more advanced classifiers, specifically \n",
    "- Support Vector Machines\n",
    "- Softmax Classifiers\n",
    "\n",
    "They function in the same way (calculating wx + b and refining the layers), but with different loss values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba31e110",
   "metadata": {},
   "source": [
    "# Generalized Structure for linear model classifiers\n",
    "\n",
    "We can define a class for both, which will share the general update structure, but utilize different loss + gradient calculations. The shared functionality will be in \n",
    "- Defining hyperparameters (learning rate, epochs)\n",
    "- Training\n",
    "- Predicting\n",
    "\n",
    "They will differ in their\n",
    "- Loss functions\n",
    "- Gradient functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad12c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        learning_rate=1e-3,\n",
    "        reg=1e-5,\n",
    "        num_iters=100,\n",
    "        batch_size=200,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        num_train, dim = X.shape\n",
    "        num_classes = (\n",
    "            np.max(y) + 1\n",
    "        )  \n",
    "        if self.W is None:\n",
    "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "        # SGD\n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "\n",
    "            indices = np.random.choice(num_train, batch_size)\n",
    "            X_batch = X[indices]\n",
    "            y_batch = y[indices]\n",
    "            \n",
    "            # Loss update\n",
    "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
    "            loss_history.append(loss)\n",
    "            self.W -= learning_rate * grad\n",
    "\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "\n",
    "        y_pred = np.argmax(X @ self.W, axis=1)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0221c61b",
   "metadata": {},
   "source": [
    "## Some more data processing!\n",
    "woohoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94bae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a395a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 500\n",
    "\n",
    "# Validation set\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "# Training set\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# Dev set (overkill)\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "y_train =y_train.squeeze()\n",
    "y_val = y_val.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622628be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: mean image\n",
    "mean_image = np.mean(X_train, axis=0).astype('uint8')\n",
    "\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image\n",
    "\n",
    "# Subsume bias dimenisio \n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f54f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f3980",
   "metadata": {},
   "source": [
    "All set for the classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c901583",
   "metadata": {},
   "source": [
    "## SVM Loss\n",
    "Now we can define the SVM loss + gradient to use with the above structure and so on.\n",
    "\n",
    "Loss is $L_i = \\sum_{j \\neq y_i}\\text{max}(0,s_j-s_{y_j}+\\Delta)$\n",
    "- Get the classes\n",
    "- Calculate the hinge loss for each\n",
    "- Sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d80cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def svm_loss_vectorized(W, X, y, reg):\n",
    "\n",
    "    loss = 0.0\n",
    "    dW = np.zeros(W.shape)  \n",
    "\n",
    "    N = len(y)    \n",
    "    Y_hat = X @ W \n",
    "\n",
    "    y_hat_true = Y_hat[range(N), y][:, np.newaxis]   \n",
    "    margins = np.maximum(0, Y_hat - y_hat_true + 1)  \n",
    "    loss = margins.sum() / N - 1 + reg * np.sum(W**2) \n",
    "    dW = (margins > 0).astype(int)   \n",
    "    dW[range(N), y] -= dW.sum(axis=1) \n",
    "    dW = X.T @ dW / N + 2 * reg * W\n",
    "    return loss, dW\n",
    "\n",
    "class LinearSVM(LinearClassifier):\n",
    "\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        return svm_loss_vectorized(self.W, X_batch, y_batch, reg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3308f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = LinearSVM()\n",
    "SVM.train(X_train, y_train)\n",
    "predictions = SVM.predict(X_test)\n",
    "print(f\"{np.sum(predictions==y_test) / len(predictions)} accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e775a0",
   "metadata": {},
   "source": [
    "## Softmax class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e284cdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax_loss_vectorized(W, X, y, reg):\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    N = X.shape[0]\n",
    "    Y_hat = X @ W\n",
    "    P = np.exp(Y_hat - Y_hat.max())     \n",
    "    P /= P.sum(axis=1, keepdims=True)    \n",
    "    loss = -np.log(P[range(N), y]).sum()\n",
    "    loss = loss / N + reg * np.sum(W**2) \n",
    "\n",
    "    P[range(N), y] -= 1                  \n",
    "    dW = X.T @ P / N + 2 * reg * W      \n",
    "    return loss, dW\n",
    "\n",
    "class Softmax(LinearClassifier):\n",
    "\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5a41ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "SM = Softmax()\n",
    "SM.train(X_train, y_train)\n",
    "predictions = SM.predict(X_test)\n",
    "print(f\"{np.sum(predictions==y_test) / len(predictions)} accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651adfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4a1e47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
